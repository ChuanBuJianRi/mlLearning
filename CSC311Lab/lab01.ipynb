{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-builder",
   "metadata": {},
   "source": [
    "# CSC311 Lab 1: Nearest Neighbours\n",
    "\n",
    "In this lab, we apply the $k$-Nearest Neighbour algorithm to classify\n",
    "hand-written digits. We will use the famous MNIST dataset to build our model.\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Write vectorized code using `numpy` to vectorize computation.\n",
    "2. Explain how images can be represented as a vector.\n",
    "3. Implement the k-Nearest Neighbour algorithm.\n",
    "4. Choose hyperparameters using a validation set.\n",
    "5. Explain the effects of varying $k$ in a $k$-Nearest Neighbour model.\n",
    "5. Report model accuracy using a test set.\n",
    "\n",
    "Please work in groups of 1-2 during the labs.\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- We use a subset of the MNIST data set [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\n",
    "  However, to demonstrate realistic data flow, we will start from raw MNIST images,\n",
    "  rather than the pre-processed data set linked.\n",
    "\n",
    "## Submission\n",
    "\n",
    "If you are working with a partner, start by creating a group on Markus.\n",
    "If you are working alone,\n",
    "click \"Working Alone\".\n",
    "\n",
    "Submit the ipynb file `lab01.ipynb` on Markus \n",
    "**containing all your solutions to the Graded Task**s.\n",
    "Your notebook file must contain your code **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the following:\n",
    "\n",
    "- Part 2. Your implementation of `dist_all`, along with the outputs of your tests (3 points)\n",
    "- Part 2. Your implementation of `predict_knn` (3 points)\n",
    "- Part 3. Your implementation of `compute_accuracy` (1 point)\n",
    "- Part 4. Your validation accuracy plot (1 point)\n",
    "- Part 4. Your explanation of the shape of this plot (1 point)\n",
    "- Part 5. Your validation accuracy plot (1 point)\n",
    "\n",
    "## Google Colab Setup\n",
    "\n",
    "We will use Google Colab to open IPython Notebook (ipynb) file. \n",
    "This tool allows us to write and execute Python code through our browser, without any environmental setup.\n",
    "\n",
    "Here are the steps to open ipynb file on Google Colab.\n",
    "\n",
    "1. Download `lab01.ipynb`, available from the Quercus course website.\n",
    "2. Click on the following link to open Google Colab: https://colab.research.google.com/\n",
    "3. Click \"Upload\", then choose the file which has been downloaded in step 1.\n",
    "\n",
    "And that's it! Now we can start writing the codes, creating the new code or text cell, etc.\n",
    "\n",
    "Here are some basic functionalities and features that you might find useful.\n",
    "\n",
    "1. Running a cell \\\n",
    "    Click the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle) \\\n",
    "    or \\\n",
    "    press SHIFT + ENTER.\n",
    "\n",
    "2. Installing libraries using Bash Commands \\\n",
    "    Although most of the commonly used libraries (e.g. NumPy, Pandas, Matplotlib) are pre-installed, \n",
    "    we may occasionally ask you to install new libraries or run other bash commands.\n",
    "    Bash commands can be run by prefixing instructions in a code cell with '!' in Google Colab (One exception: 'cd' command can be run by prefixing with '%'), e.g. `!pip install [package name]`\n",
    "\n",
    "3. Mounting Google Drive \\\n",
    "    You may optionally mount Google Drive.\n",
    "    Click the files button on the left pane, then click on 'mount drive' button (looks like a file icon with a google drive logo). \\\n",
    "    or \\\n",
    "    Run the following code snippet:\n",
    "    ```\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ```\n",
    "    By mounting the drive, we can use any files or folders in our drive by using the path as follows:\n",
    "    ```\n",
    "    /content/drive/MyDrive/[folder name]\n",
    "    ```\n",
    "    For example, we can read the csv file uploaded in the drive using Pandas library as follows:\n",
    "    ```\n",
    "    pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')\n",
    "    ```\n",
    " \n",
    "## Part 1. Data and Numpy\n",
    "\n",
    "**Task:** We will use a subset of MNIST image files, available here: [https://www.cs.toronto.edu/~lczhang/311/lab01/data.zip](https://www.cs.toronto.edu/~lczhang/311/lab01/data.zip)\n",
    "Start by downloading and unzipping this dataset on your own machine,\n",
    "so that you can understand the directory structure.\n",
    "How many folders are in the zip file? How many image files are in each folder?\n",
    "How is each image labeled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-climate",
   "metadata": {},
   "source": [
    "We will also need to download the data to Google Colab's working directory.\n",
    "Fortunately, Google Colab allows users to run certain bash commands. \n",
    "Bash commands need have a `!` at the beginning.\n",
    "We will run these two lines of code to download and unzip our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the lab data files.\n",
    "!wget https://www.cs.toronto.edu/~lczhang/311/lab01/data.zip\n",
    "\n",
    "# Unzip the zip file.\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-region",
   "metadata": {},
   "source": [
    "Now that the data has been downloaded to the root directory that Colab has access to,\n",
    "we can begin reading the files in Python.\n",
    "We will use Python's `PIL` and `matplotlib` libraries to read and\n",
    "display these MNIST hand-written digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(\"mnist-subset/0/16585.png\")\n",
    "plt.imshow(img, cmap=\"gray\") # display image as a greyscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-mirror",
   "metadata": {},
   "source": [
    "With any machine learning problem, it is important to build\n",
    "an intuitive understanding of what our data looks like.\n",
    "\n",
    "**Task:**\n",
    "Display several more images, each of a different digit, by\n",
    "modifying `filename` below.\n",
    "This is so that you build an intuitive understanding of the\n",
    "variations between different digits and different images.\n",
    "Your TA will check that the `filename` variable has been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"mnist-subset/0/16585.png\" # modify me!\n",
    "plt.imshow(Image.open(filename), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-china",
   "metadata": {},
   "source": [
    "As we mentioned during lecture, we will represent each input as a\n",
    "vector of features. In this case, we will need to turn each image\n",
    "into a vector of features. The features we use will be the pixel\n",
    "intensities at each of the 784 pixel locations of the image.\n",
    "\n",
    "The piece of code below uses the Python linear algebra library called\n",
    "`numpy` to represent each image as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = Image.open(\"mnist-subset/0/16585.png\")\n",
    "pixels = np.array(img)\n",
    "\n",
    "# Display the *shape* of this numpy array\n",
    "print(pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-holly",
   "metadata": {},
   "source": [
    "Here, `pixels` is a 2D numpy array containing the pixel intensities\n",
    "of one image. The elements of `pixels` can be *indexed*.\n",
    "For examples `pixels[0, 0]` indexes the first row and column.\n",
    "Additionally, similar to python lists, numpy arrays support *slicing*:\n",
    "e.g. `pixels[1:3, 0]` and `pixels[200:, :]`.\n",
    "\n",
    "**Task:** Print the entire top row of pixels of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-maintenance",
   "metadata": {},
   "source": [
    "As mentioned in lecture 1, we would like to represent this image as a\n",
    "*vector*. To do so, we can use the `reshape` method to change the shape\n",
    "of this matrix to a 1D vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = pixels.reshape(784)\n",
    "# alternatively\n",
    "# pixels = pixels.reshape(-1) # infer the shape of the single dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-karma",
   "metadata": {},
   "source": [
    "One thing that trips up a lot of students is that numpy does *not* distinguish\n",
    "between row and column vectors. Here, `pixels` is simply a vector.\n",
    "\n",
    "We will show more `numpy` functionalities as we need them. If you prefer to first\n",
    "go through a `numpy` tutorial before proceeding, we recommend this one:\n",
    "[https://cs231n.github.io/python-numpy-tutorial/](https://cs231n.github.io/python-numpy-tutorial/)\n",
    "\n",
    "**If the pace in which we are introducing numpy is too fast for you, consider\n",
    "taking CSC338 concurrently, or before attempting CSC311.**\n",
    "\n",
    "Now that we know how to read an image into a vector, we can read in \n",
    "the entire set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # for interacting with the file system\n",
    "\n",
    "def img_to_vec(img):\n",
    "    \"\"\"Return a vector representation of an MNIST image file\"\"\"\n",
    "    img = Image.open(img)\n",
    "    return np.array(img).reshape(-1)\n",
    "\n",
    "# D will house our data \n",
    "D = []\n",
    "\n",
    "# Iterate over all files that match the pattern \"mnist-subset/*/*.png\"\n",
    "# and add its information to `D`. We will sort the filenames so that we get\n",
    "# a consistent set of files in the training, validation, and test sets.\n",
    "for file in sorted(glob.glob(\"mnist-subset/*/*.png\")): \n",
    "    x = img_to_vec(file)   # vector input\n",
    "    t = file.split(\"/\")[1] # find out the target label by reading the file path\n",
    "    D.append((x, t),) # add this to the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-issue",
   "metadata": {},
   "source": [
    "This representation of the labelled data `D` as a list of tuples of `(features, labels)`\n",
    "is not efficient (does not allow for parallelization).\n",
    "We will change the representation shortly.\n",
    "For now, let's first separate the data into training, validation, and test sets.\n",
    "\n",
    "(In the code below, why do you think we need to shuffle `D`? Can you think of other approaches to splitting the training/validation/test data?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you re-run this cell without the also re-running the previous\n",
    "# cell block, then your train/validation/test splits will differ from\n",
    "# everyone else's\n",
    "\n",
    "import random\n",
    "random.seed(5)         # set the random seed.\n",
    "random.shuffle(D)\n",
    "D_train = D[:4000]     # the training set\n",
    "D_valid = D[4000:4500] # the validation set\n",
    "D_test  = D[4500:]     # the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-science",
   "metadata": {},
   "source": [
    "To take full advantage of `numpy`, we will express our computations as matrix operations as\n",
    "much as we can. For our nearest-neighbour model, this means that we would like to express distance\n",
    "computations using matrix operations as much as we can. We will thus store our training data\n",
    "in a **matrix**, and our target values in a **vector**.\n",
    "\n",
    "In other words, the following code takes the training data `D_train`, and produces:\n",
    "\n",
    "1. a **data matrix** `X_train`, stored as a numpy array of shape `[4000, 784]`. In this data matrix, each row `X_train[i, :]` represents a single **image** in the training data.\n",
    "2. a **target vector** `t_train`, stored as a numpy array of shape `[4000]`.  In this vector, each `t[i]` contains the target for image $i$ in the training data.\n",
    "\n",
    "Please take a look at the documentation for `np.stack` to see how the first line of code works [https://numpy.org/doc/stable/reference/generated/numpy.stack.html](https://numpy.org/doc/stable/reference/generated/numpy.stack.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct X_train by first producing a list of vectors\n",
    "# here, each element x_vectors[i] is a vector representation of an image\n",
    "x_vectors = [x for (x, t) in D_train]\n",
    "X_train = np.stack(x_vectors)\n",
    "\n",
    "# construct t_train by first producing a list of targets\n",
    "targets = [t for (x, t) in D_train]\n",
    "t_train = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-gossip",
   "metadata": {},
   "source": [
    "**Task** Produce the corresponding data matrix and target vectors `X_valid`, `t_valid`,\n",
    "`X_test`, `t_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = None\n",
    "t_valid = None\n",
    "X_test = None\n",
    "t_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-exhibition",
   "metadata": {},
   "source": [
    "## Part 2. Distance Computation\n",
    "\n",
    "In order to use the nearest neighbour model to predict MNIST digit labels, we need\n",
    "to be able to compute *Euclidean distances* between images represented as vectors\n",
    "(specifically, `numpy` arrays of shape `(784,)`).\n",
    "In this section, we will focus on this distance computation.\n",
    "Euclidean distance computation should be straightforward for most students familiar\n",
    "with Python; the challenge is to write **vectorized** code in `numpy` that will allow\n",
    "us to parallelize many distance computations all at once.\n",
    "\n",
    "**Task:** Start by writing a function that computes the squared Euclidean distance\n",
    "between two images, both represented as `numpy` arrays of shape `(784,)`. That is, compute for vectors ${\\bf v}$ and ${\\bf x}$:\n",
    "\n",
    "$$\\textrm{dist}({\\bf v}, {\\bf x}) = \\sum_{j=1}^{D} (v_j - x_j)^2$$\n",
    "\n",
    "Notice that we will use squared Euclidean distance as our distance measure, rather\n",
    "than the Euclidean distance itself. (Why is this okay?)\n",
    "\n",
    "**Do not use any loops.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_single(v, x):\n",
    "    \"\"\"\n",
    "    Compute the squared Euclidean distance between vectors `v` and `x`.\n",
    "\n",
    "    Parameters:\n",
    "        `v` - a numpy array (vector) representing an MNIST image, shape (784,)\n",
    "        `x` - a numpy array (vector) representing an MNIST image, shape (784,)\n",
    "\n",
    "    Returns: a scalar value representing their squared Euclidean distance\n",
    "    \"\"\"\n",
    "    diff = x - v # compute a difference vector  (x-v)\n",
    "    sqdiff = None # TODO: compute element-wise **square** of each element in `diff`\n",
    "                  # In numpy, operations like `diff + 2`, `diff - 2`, and `diff ** 3` \n",
    "                  # are all computed element-wise.\n",
    "    sumval = None # TODO: compute the sum of the elements of `sqdiff`.\n",
    "                  # You may find one of the functions from this list useful:\n",
    "                  # https://numpy.org/doc/stable/reference/routines.math.html\n",
    "    return sumval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-finland",
   "metadata": {},
   "source": [
    "Since we did not write any loops, each step of the computation (e.g. `x - v`) can be done\n",
    "in parallel (e.g. `x[1] - v[1]` can be computed in parallel with `x[2] - v[2]`).\n",
    "\n",
    "To test the above function, let's try and compute some distances to see if the result\n",
    "is what we expect.\n",
    "\n",
    "**Task**: Run and add to these tests. Add at least 3 new tests. You will likely want to write\n",
    "more tests to test your code more systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an image (vector) that is all black\n",
    "black = np.zeros(784)\n",
    "\n",
    "# construct an image (vector) that is all white\n",
    "white = np.ones(784)\n",
    "\n",
    "# construct an image (vector) that is all black except a single white pixel\n",
    "dot = np.zeros(784)\n",
    "dot[100] = 1\n",
    "\n",
    "print(\"This should be 0:\", dist_single(black, black))\n",
    "print(\"This should be 0:\", dist_single(dot, dot))\n",
    "print(\"This should be 0:\", dist_single(white, white))\n",
    "print(\"This should be 1:\", dist_single(black, dot))\n",
    "print(\"This should be 784:\", dist_single(black, white))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-slovenia",
   "metadata": {},
   "source": [
    "To continue, we will parallelize distance computations further,\n",
    "by writing a new function \n",
    "that computes the distance between a single image vector `v` and\n",
    "all the images in a data matrix `X`. Eventually, we will use this function\n",
    "to find the closest images to a new image `v` in the training set `X_train`.\n",
    "\n",
    "**Graded Task:** Write a function that takes a vector `v` (representing an MNIST image),\n",
    "and computes the squared Euclidean distance between `v` and every data point in\n",
    "the training set `X_train`.  **Do not use any loops.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_all(v, X):\n",
    "    \"\"\"\n",
    "    Compute the squared Euclidean distance between an image `v` (vector) and the\n",
    "    images in the data matrix `X`.\n",
    "\n",
    "    Parameters:\n",
    "        `v` - a numpy array (vector) representing an MNIST image, shape (784,)\n",
    "        `X` - a data matrix representing a set of MNIST image, shape (N, 784)\n",
    "\n",
    "    Returns: a vector of squared Euclidean distances between `v` and each image in `X`,\n",
    "             shape (N,)\n",
    "    \"\"\"\n",
    "\n",
    "    diff = X - v # compute difference vectors (x-v), stored in a matrix\n",
    "                 # to do so, we use an idea in numpy called broadcasting\n",
    "                 # https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "    # Here, diff has a shape of (N, 784), and each diff[i, :] is a vector\n",
    "    # equivalent to (X[i, :] - v)\n",
    "\n",
    "    sqdiff = None # TODO: compute element-wise **square** of each element in `diff`\n",
    "                  # In numpy, operations like `diff + 2`, `diff - 2`, and `diff ** 3` \n",
    "                  # are all computed element-wise.\n",
    "                  # Hint: Does does approach you used in `dist_single` still work?\n",
    "\n",
    "    sumval = None # TODO: compute the sum of the elements of `sqdiff` along each row\n",
    "                  # Hint: What additional parameter do you need to add to the approach\n",
    "                  # you used in `dist_single`? (Assuming you used np.sum)\n",
    "    return sumval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-integration",
   "metadata": {},
   "source": [
    "As before, let's try and compute some distances to see if it makes sense.\n",
    "Please included the output of these tests in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an image (vector) that is all black\n",
    "black = np.zeros(784)\n",
    "\n",
    "# construct an image (vector) that is all white\n",
    "white = np.ones(784)\n",
    "\n",
    "# construct an image (vector) that is all black except a single white pixel\n",
    "dot = np.zeros(784)\n",
    "dot[100] = 1\n",
    "\n",
    "# combine all three images into a data matrix\n",
    "X = np.stack([black, dot, white])\n",
    "\n",
    "print(dist_all(dot, X)) # Is this what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write more tests to convince yourself that your code works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-silence",
   "metadata": {},
   "source": [
    "## Part 3. Nearest Neighbour\n",
    "\n",
    "Now that we know how to compute distances, we have almost everything we need\n",
    "to be able to implement the k-Nearest Neighbour algorithm. Our goal in this\n",
    "part is to be able to make a prediction (labels 0, 1, ... 10) given an image\n",
    "$v$ represented as a vector. To make such a prediction, we will need to \n",
    "follow three steps:\n",
    "\n",
    "1. Compute the distance between `v` to every image in our training set. We already\n",
    "   have a helper function `dist_all` that does this for us.\n",
    "2. Given the vector of distances returned by `dist_all`, find the indices of the `k` \n",
    "   closets images in `X` to `v`.\n",
    "3. Given these `k` indices, find the most common label amongst these `k` images.\n",
    "   If there is a tie, choose any of the most common labels.\n",
    "\n",
    "**Graded Task**: Let's write the function `predict_knn()` that makes such predictions.\n",
    "We recommend that you follow these above three steps, but if you prefer a different\n",
    "approach, that's fine too as long as it works and is fast.\n",
    "\n",
    "(This portion of the lab should be programming that only uses concepts\n",
    "taught in courses like CSC148. It is normal to have to think a bit to be able\n",
    "to write this code. However, if you find this type of coding extremely challenging, \n",
    "you will likely benefit from further computer science preparation before taking CSC311.\n",
    "Consider taking any course that requires you to practice writing code to solve problems\n",
    "and come back to CSC311 in a future year.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(v, X_train=X_train, t_train=t_train, k=1):\n",
    "    \"\"\"\n",
    "    Returns a prediction using the k-NN\n",
    "\n",
    "    Parameters:\n",
    "        `v` - a numpy array (vector) representing an MNIST image, shape (784,)\n",
    "        `X_train` - a data matrix representing a set of MNIST image, shape (N, 784)\n",
    "        `t_train` - a vector of ground-truth labels, shape (N,)\n",
    "        `k` - a positive integer 1 < k <= N, describing the number of closest images\n",
    "              to consider as part of the knn algorithm\n",
    "\n",
    "    Returns: \n",
    "        A single number `i` between 0 and 9, representing the digit \n",
    "    \"\"\"\n",
    "    # Step 1. compute the distances between v and every element of X\n",
    "    dists = dist_all(v, X_train)\n",
    "\n",
    "    # Step 2. find the indices of the k-nearest neighbours\n",
    "\n",
    "    # Hint: You may wish to sort the distances in `dists`. But how should you\n",
    "    # do this sorting while keeping track of the indices? You may find\n",
    "    # the functions \"enumerate\" (or \"zip\"), and \"sorted\" helpful.\n",
    "    # Alternatively, you may choose to use a function like \"np.argsort\"\n",
    "    indices = None\n",
    "\n",
    "    # Step 3. find the most common target label amongst these indices\n",
    "\n",
    "    ts = t_train[np.array(indices)] # get the target labels for these indices\n",
    "    # NOTE: the above code uses numpy's ability to take a *list of indices*\n",
    "    # For example, t_train[np.array([1, 10, 4])] will produce a numpy array\n",
    "    # with the elements: t_train[1], t_train[10], t_train[4]\n",
    "\n",
    "    # What else do you need to do to produce the prediction?\n",
    "\n",
    "    prediction = None\n",
    "\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-programming",
   "metadata": {},
   "source": [
    "**Task**: Use the function `predict_knn()` to compute the prediction\n",
    "for `X_valid[5]`, displayed below. Choose a value of `k` of your choice.\n",
    "Is the prediction correct?\n",
    "You are also welcome to change the value of `idx` to explore how\n",
    "the prediction changes for different images in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "plt.imshow(X_valid[idx].reshape(28, 28), cmap=\"gray\") # visualize this image\n",
    "\n",
    "prediction = None # TODO\n",
    "\n",
    "print(\"Ground truth:\", t_valid[idx])\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-macintosh",
   "metadata": {},
   "source": [
    "**Graded Task**: We would like to summarize how a k-NN model performs on a data set.\n",
    "Complete the function `compute_accuracy()`, which computes the accuracy of\n",
    "the k-NN model across a data set.\n",
    "\n",
    "In theory, we could attempt to vectorize this computation. However, the sorting\n",
    "operation in `predict_knn` is challenging to parallelize.\n",
    "For simplicity, let's use a loop to iterate over `X_new` and `t_new`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X_new, t_new, X_train=X_train, t_train=t_train, k=1):\n",
    "    \"\"\"\n",
    "    Returns the accuracy (proportion of correct predictions) on the data set\n",
    "    `X_new` and ground truth `t_new`.\n",
    "\n",
    "    Parameters:\n",
    "        `X_new` - a data matrix representing MNIST images that we would like to\n",
    "                  make predictions for, shape (N', 784)\n",
    "        `t_new` - a data matrix representing ground truth labels for images in X_new,\n",
    "                  shape (N',)\n",
    "        `X_train` - a data matrix representing a set of MNIST image in the training set,\n",
    "                    shape (N, 784)\n",
    "        `t_train` - a vector of ground-truth labels for images in X_train,\n",
    "                    shape (N,)\n",
    "        `k` - a positive integer 1 < k <= N, describing the number of closest images\n",
    "              to consider as part of the knn algorithm\n",
    "\n",
    "    Returns: the proportion of correct predictions (between 0 and 1)\n",
    "    \"\"\"\n",
    "\n",
    "    num_predictions = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for i in range(X_new.shape[0]): # iterate over each image index in X_new\n",
    "        v = X_new[i] # image vector\n",
    "        t = t_new[i] # prediction target\n",
    "\n",
    "        y = None # TODO\n",
    "\n",
    "        num_correct += 0 # TODO: change this!\n",
    "\n",
    "        num_predictions += 1\n",
    "\n",
    "    return num_correct / num_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-pacific",
   "metadata": {},
   "source": [
    "Let's estimate the training accuracy for $k=1$.\n",
    "To make the computation faster, we will use a subset of the training data.\n",
    "The training accuracy should be either equal to or close to 100%.\n",
    "(Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(X_train[:500], t_train[:500], X_train=X_train, t_train=t_train, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-rendering",
   "metadata": {},
   "source": [
    "## Part 4. Hyperparameter Tuning\n",
    "\n",
    "In this section, we will fine-tune the hyperparameter parameter $k$ for our data set.\n",
    "To do so, we will need to compute the validation accuracy across our data set for different choices of $k$.\n",
    "\n",
    "**Graded Task:** Compute the validation accuracy for every value of $k$ between 1 and 10, inclusive.\n",
    "Store these values in an array called `valid_acc`, so that `valid_acc[k-1]` stores the validation\n",
    "accuracy for this choice of $k$.\n",
    "\n",
    "(You may optionally find a faster way to compute the validation accuracies by avoiding \n",
    "repeated distance computations.)\n",
    "\n",
    "Since this code will not be run repeatedly, this code is much less important to optimize compared\n",
    "to functions like `dist_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc = []\n",
    "\n",
    "for k in range(1, 11):\n",
    "    acc = 0 # TODO\n",
    "    valid_acc.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-coordination",
   "metadata": {},
   "source": [
    "If your code above is correct, you should obtain a validation accuracy of approximately ~20-30%. This is\n",
    "better than random, but we will be able to do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validatation Accuracy for an Unnormalized kNN model\")\n",
    "plt.plot(range(1, 11), valid_acc)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-tooth",
   "metadata": {},
   "source": [
    "**Task** What is the optimal value of $k$ we should choose? Recall that we should make this choice using\n",
    " the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-capacity",
   "metadata": {},
   "source": [
    "**Graded Task** Explain the shape of the above plot. Why does validation accuracy increase at first, and then decrease?\n",
    "Recall that we used words like \"overfitting\" and \"underfitting\" during lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-story",
   "metadata": {},
   "source": [
    "## Part 5. Normalizing the data\n",
    "\n",
    "In this part, we will show why it is important to **normalize our data** when using a nearest neighbour model.\n",
    "In particular, we will normalize each feature (each pixel, or each of the 784 coordinates in `X_train`)\n",
    "so that it has 0 mean and standard deviation 1.\n",
    "In other words, we will normalize (standardize) the data matrix `X_train` so that each of its *columns*\n",
    "has a mean 0 and standard deviation 1.\n",
    "\n",
    "Let's start by computing the means and standard deviations of each pixel (each column of the data matrix).\n",
    "We will use the training set to compute these values.\n",
    "(Why do you think it would be a bad idea to instead use all the available data that we have, including the test set?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "print(X_mean.shape)\n",
    "print(X_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-tonight",
   "metadata": {},
   "source": [
    "Now, we can transform the input data by subtracting the mean and dividing by the standard deviation\n",
    "in the training set. We again use broadcasting ([https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "to parallelize our computation. `X_train` has shape `(N, D)` and `X_mean` has shape `(D,)`, \n",
    "and so `X_train - X_mean` will have shape `(N, D)` with each row of its result corresponding\n",
    "to the value of `X_train[row, :] - X_mean`. The division operation is broadcasted similarly.\n",
    "Additionally, we add a positive value to the denominator in case some of the standard deviation\n",
    "values are exactly 0, to avoid divisions by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "X_train_norm = (X_train - X_mean) / (X_std + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-context",
   "metadata": {},
   "source": [
    "**Task:** Apply the same transformation as above to data in the validation and test sets (i.e., subtract `X_mean` and divide by `X_std`).\n",
    "Note that we should *not* recompute means and standard deviations. Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_norm = None\n",
    "X_test_norm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-madagascar",
   "metadata": {},
   "source": [
    "**Graded Task:** Now, using this normalized data let's compute the validation accuracies for values of $k$ between 1 and 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc_norm = []\n",
    "\n",
    "for k in range(1, 11):\n",
    "    acc = 0 # TODO\n",
    "    valid_acc_norm.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-poison",
   "metadata": {},
   "source": [
    "If your code above is correct, you should obtain a much higher validation accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validatation Accuracy for a Normalized kNN model\")\n",
    "plt.plot(range(1, 11), valid_acc_norm)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-finger",
   "metadata": {},
   "source": [
    "## Part 6. Reporting Test Accuracy\n",
    "\n",
    "Following machine learning best practices, we use the test set exactly once see how our model\n",
    "might perform on new data that it has never seen before.\n",
    "\n",
    "**Task**: Report the test accuracy for best model that we have so far.\n",
    "(What does \"best\" mean in this case?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
